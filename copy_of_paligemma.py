# -*- coding: utf-8 -*-
"""Copy of PaliGemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBmU7VNDRXPjhctFiBHqimA446I0i6qe

# ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡
# ğŸ‘‰ğŸ‘‰ğŸ‘‰[æˆ‘çš„å“”å“©å“”å“©é¢‘é“](https://space.bilibili.com/3493277319825652)
# ğŸ‘‰ğŸ‘‰ğŸ‘‰[æˆ‘çš„YouTubeé¢‘é“](https://www.youtube.com/@AIsuperdomain)

# ä½¿ç”¨ ğŸ¤— transformers æ¥è¿è¡Œ PaliGemma æ¨¡å‹

PaliGemma æ˜¯ Google å‘å¸ƒçš„æ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚åœ¨è¿™ä¸ªç¬”è®°
ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— transformers è¿›è¡Œ PaliGemma çš„æ¨ç†ã€‚
é¦–å…ˆï¼Œç”±äºæˆ‘ä»¬éœ€è¦ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ ğŸ¤— transformers åŠå…¶ä»–ç›¸å…³åº“ï¼Œæ‰€ä»¥è¯·ä½¿ç”¨æ›´æ–°æ ‡å¿—æ¥å®‰è£…ä»¥ä¸‹åº“ã€‚
"""

# !pip install -q -U accelerate bitsandbytes git+https://github.com/huggingface/transformers.git

import torch
import numpy as np
from PIL import Image
import requests

input_text = "What color is the flower that bee is standing on?"
input_text = "èŠ±ä¸Šé¢æ˜¯ä»€ä¹ˆåŠ¨ç‰©"
img_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true"
input_image = Image.open(requests.get(img_url, stream=True).raw)

"""[link text](https://)å›¾ç‰‡å¦‚ä¸‹æ‰€ç¤ºã€‚

![](https://github.com/win4r/AISuperDomain/assets/42172631/7ded8001-ae5f-464d-bef0-dbda2eb5ed16)

ä½ å¯ä»¥åƒä¸‹é¢è¿™æ ·åŠ è½½ PaliGemma æ¨¡å‹å’Œå¤„ç†å™¨ã€‚

1.   åˆ—è¡¨é¡¹
2.   åˆ—è¡¨é¡¹
"""

from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_id = "leo009/paligemma-3b-mix-224"
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)
processor = PaliGemmaProcessor.from_pretrained(model_id)

"""å¤„ç†å™¨ä¼šåŒæ—¶é¢„å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¼ é€’å®ƒä»¬ã€‚

---

"""

inputs = processor(text=input_text, images=input_image,
                  padding="longest", do_convert_rgb=True, return_tensors="pt").to("cuda")
model.to(device)
inputs = inputs.to(dtype=model.dtype)

"""æˆ‘ä»¬å¯ä»¥ä¼ å…¥æˆ‘ä»¬é¢„å¤„ç†è¿‡çš„è¾“å…¥ã€‚

> æ·»åŠ å¼•ç”¨å—

"""

with torch.no_grad():
  output = model.generate(**inputs, max_length=496)

print(processor.decode(output[0], skip_special_tokens=True))

"""[link text](https://)## ä»¥4æ¯”ç‰¹åŠ è½½æ¨¡å‹

ä½ è¿˜å¯ä»¥ä»¥4æ¯”ç‰¹å’Œ8æ¯”ç‰¹åŠ è½½æ¨¡å‹ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä»¥èŠ‚çœå†…å­˜ã€‚
é¦–å…ˆï¼Œåˆå§‹åŒ– `BitsAndBytesConfig`ã€‚

"""

# from transformers import BitsAndBytesConfig
# import torch
# nf4_config = BitsAndBytesConfig(
#    load_in_4bit=True,
#    bnb_4bit_quant_type="nf4",
#    bnb_4bit_use_double_quant=True,
#    bnb_4bit_compute_dtype=torch.bfloat16
# )

# """:ç°åœ¨æˆ‘ä»¬å°†é‡æ–°åŠ è½½æ¨¡å‹ï¼Œä½†å°†ä¸Šè¿°å¯¹è±¡ä½œä¸º `quantization_config` ä¼ å…¥ã€‚

# """

# from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor
# import torch

# device="cuda"
# model_id = "leo009/paligemma-3b-mix-224"
# model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,
#                                                           quantization_config=nf4_config, device_map={"":0})
# processor = PaliGemmaProcessor.from_pretrained(model_id)

# with torch.no_grad():
#   output = model.generate(**inputs, max_length=496)

# print(processor.decode(output[0], skip_special_tokens=True))

# """# ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡
# # ğŸ‘‰ğŸ‘‰ğŸ‘‰[æˆ‘çš„å“”å“©å“”å“©é¢‘é“](https://space.bilibili.com/3493277319825652)
# # ğŸ‘‰ğŸ‘‰ğŸ‘‰[æˆ‘çš„YouTubeé¢‘é“](https://www.youtube.com/@AIsuperdomain)
# """